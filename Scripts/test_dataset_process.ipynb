{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa34d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATASET PROCESSING PIPELINE\n",
      "\n",
      "[STEP 1] Loading data and saved artifacts...\n",
      " Test data loaded: (50000, 10)\n",
      " Auxiliary data loaded\n",
      " Scaler loaded\n",
      " Train geo info loaded\n",
      " KNN feature statistics loaded (8 features)\n",
      " Numerical features list loaded (36 features)\n",
      " All final features list loaded (89 features)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Utils')\n",
    "from tools import create_auxiliary_location_features\n",
    "print(\"TEST DATASET PROCESSING PIPELINE\")\n",
    "\n",
    "print(\"\\n[STEP 1] Loading data and saved artifacts...\")\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('../Dataset/test.csv')\n",
    "print(f\" Test data loaded: {test_df.shape}\")\n",
    "\n",
    "# Load auxiliary data\n",
    "df_hdb = pd.read_csv('../Dataset/auxiliary-data/sg-hdb-block-details.csv')\n",
    "df_mrt = pd.read_csv('../Dataset/auxiliary-data/sg-mrt-stations.csv')\n",
    "df_hawker = pd.read_csv('../Dataset/auxiliary-data/sg-gov-hawkers.csv')\n",
    "df_primary = pd.read_csv('../Dataset/auxiliary-data/sg-primary-schools.csv')\n",
    "df_secondary = pd.read_csv('../Dataset/auxiliary-data/sg-secondary-schools.csv')\n",
    "df_malls = pd.read_csv('../Dataset/auxiliary-data/sg-shopping-malls.csv')\n",
    "print(f\" Auxiliary data loaded\")\n",
    "# Load saved artifacts from training\n",
    "with open('../Dataset/features_standard_scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "print(f\" Scaler loaded\")\n",
    "\n",
    "with open('../Dataset/train_geo_info.pkl', 'rb') as f:\n",
    "    train_geo_info = pickle.load(f)\n",
    "print(f\" Train geo info loaded\")\n",
    "\n",
    "with open('../Dataset/knn_feature_stats.pkl', 'rb') as f:\n",
    "    knn_feature_stats = pickle.load(f)\n",
    "print(f\" KNN feature statistics loaded ({len(knn_feature_stats)} features)\")\n",
    "\n",
    "with open('../Dataset/numerical_features.json', 'r') as f:\n",
    "    numerical_features = json.load(f)\n",
    "print(f\" Numerical features list loaded ({len(numerical_features)} features)\")\n",
    "\n",
    "with open('../Dataset/all_final_features.json', 'r') as f:\n",
    "    all_final_features = json.load(f)\n",
    "print(f\" All final features list loaded ({len(all_final_features)} features)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "171a451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Basic feature engineering...\n",
      "✓ FLAT_TYPE cleaned: ['3-room' '4-room' '5-room' 'executive' '2-room' '1-room'\n",
      " 'multi generation']\n",
      "✓ Time features extracted\n",
      "✓ REMAINING_AGE calculated\n",
      "✓ Floor level features extracted\n",
      " String standardization completed\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Basic Feature Engineering (Same as Train)\n",
    "print(\"\\n[STEP 2] Basic feature engineering...\")\n",
    "\n",
    "test_processed = test_df.copy()\n",
    "\n",
    "# 2.1 Clean FLAT_TYPE\n",
    "test_processed['FLAT_TYPE'] = test_processed['FLAT_TYPE'].str.replace(' room', '-room', case=False)\n",
    "test_processed['FLAT_TYPE'] = test_processed['FLAT_TYPE'].str.replace(' ROOM', '-ROOM', case=False)\n",
    "print(f\"✓ FLAT_TYPE cleaned: {test_processed['FLAT_TYPE'].unique()}\")\n",
    "\n",
    "# 2.2 Extract time features\n",
    "test_processed['TRANSACTION_YEAR'] = test_processed['MONTH'].str[:4].astype(int)\n",
    "test_processed['TRANSACTION_MONTH'] = test_processed['MONTH'].str[5:].astype(int)\n",
    "print(f\"✓ Time features extracted\")\n",
    "\n",
    "# 2.3 Calculate REMAINING_AGE\n",
    "test_processed['REMAINING_AGE'] = 99 - test_processed['TRANSACTION_YEAR'] + test_processed['LEASE_COMMENCE_DATA']\n",
    "print(f\"✓ REMAINING_AGE calculated\")\n",
    "\n",
    "# 2.4 Extract floor level information\n",
    "test_processed['FLOOR_LEVEL_LOW'] = test_processed['FLOOR_RANGE'].str.split(' to ').str[0].astype(int)\n",
    "test_processed['FLOOR_LEVEL_HIGH'] = test_processed['FLOOR_RANGE'].str.split(' to ').str[1].astype(int)\n",
    "test_processed['FLOOR_LEVEL_MID'] = (test_processed['FLOOR_LEVEL_LOW'] + test_processed['FLOOR_LEVEL_HIGH']) / 2\n",
    "print(f\"✓ Floor level features extracted\")\n",
    "\n",
    "# 2.5 String standardization\n",
    "test_processed['STREET'] = test_processed['STREET'].str.strip().str.lower()\n",
    "test_processed['TOWN'] = test_processed['TOWN'].str.strip().str.lower()\n",
    "test_processed['BLOCK'] = test_processed['BLOCK'].str.strip().str.upper()\n",
    "print(f\" String standardization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810dab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Categorical encoding...\n",
      "✓ FLAT_TYPE encoded\n",
      "FLAT_MODEL encoded: 21 columns\n",
      "✓ TOWN encoded: 26 columns\n",
      " Encoded features concatenated. Shape: (50000, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 3] Categorical encoding...\")\n",
    "\n",
    "# 3.1 FLAT_TYPE - Ordinal Encoding\n",
    "flat_type_order = [\n",
    "    '1-room', '2-room', '3-room', '4-room', '5-room', \n",
    "    'executive', 'multi generation'\n",
    "]\n",
    "flat_type_encoder = OrdinalEncoder(categories=[flat_type_order])\n",
    "test_processed['FLAT_TYPE_ENCODED'] = flat_type_encoder.fit_transform(test_processed[['FLAT_TYPE']])\n",
    "print(f\"✓ FLAT_TYPE encoded\")\n",
    "\n",
    "# 3.2 FLAT_MODEL - OneHot Encoding\n",
    "\n",
    "train_flat_models = [\n",
    "    '2 room', '3gen', 'adjoined flat', 'apartment', 'dbss', 'improved',\n",
    "    'improved maisonette', 'maisonette', 'model a', 'model a maisonette',\n",
    "    'model a2', 'multi generation', 'new generation', 'premium apartment',\n",
    "    'premium apartment loft', 'premium maisonette', 'simplified',\n",
    "    'standard', 'terrace', 'type s1', 'type s2'\n",
    "]\n",
    "\n",
    "encoder_flat_model = OneHotEncoder(sparse_output=False, categories=[train_flat_models], handle_unknown='ignore')\n",
    "encoded_flat_model = encoder_flat_model.fit_transform(test_processed[['FLAT_MODEL']])\n",
    "flat_model_cols = [f'FLAT_MODEL_{cat}' for cat in train_flat_models]\n",
    "encoded_flat_model_df = pd.DataFrame(encoded_flat_model, columns=flat_model_cols, index=test_processed.index)\n",
    "print(f\"FLAT_MODEL encoded: {encoded_flat_model_df.shape[1]} columns\")\n",
    "\n",
    "# 3.3 TOWN - OneHot Encoding\n",
    "train_towns = [\n",
    "    'ang mo kio', 'bedok', 'bishan', 'bukit batok', 'bukit merah',\n",
    "    'bukit panjang', 'bukit timah', 'central area', 'choa chu kang',\n",
    "    'clementi', 'geylang', 'hougang', 'jurong east', 'jurong west',\n",
    "    'kallang/whampoa', 'marine parade', 'pasir ris', 'punggol',\n",
    "    'queenstown', 'sembawang', 'sengkang', 'serangoon', 'tampines',\n",
    "    'toa payoh', 'woodlands', 'yishun'\n",
    "]\n",
    "\n",
    "encoder_town = OneHotEncoder(sparse_output=False, categories=[train_towns], handle_unknown='ignore')\n",
    "encoded_town = encoder_town.fit_transform(test_processed[['TOWN']])\n",
    "town_cols = [f'TOWN_{cat}' for cat in train_towns]\n",
    "encoded_town_df = pd.DataFrame(encoded_town, columns=town_cols, index=test_processed.index)\n",
    "print(f\"✓ TOWN encoded: {encoded_town_df.shape[1]} columns\")\n",
    "\n",
    "# Concatenate encoded features\n",
    "test_processed = pd.concat([test_processed, encoded_flat_model_df, encoded_town_df], axis=1)\n",
    "print(f\" Encoded features concatenated. Shape: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab8454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Merging with HDB location data...\n",
      "HDB data merged. Missing coordinates: 0/50000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 4] Merging with HDB location data...\")\n",
    "\n",
    "# Prepare HDB data\n",
    "important_cols = ['BLOCK', 'ADDRESS', 'LATITUDE', 'LONGITUDE', 'POSTAL_CODE', 'MAX_FLOOR']\n",
    "df_hdb_processed = df_hdb[important_cols].copy()\n",
    "df_hdb_processed['ADDRESS'] = df_hdb_processed['ADDRESS'].str.strip().str.lower()\n",
    "df_hdb_processed['BLOCK'] = df_hdb_processed['BLOCK'].str.strip().str.upper()\n",
    "\n",
    "# Merge\n",
    "test_processed = test_processed.merge(\n",
    "    df_hdb_processed,\n",
    "    left_on=['BLOCK', 'STREET'],\n",
    "    right_on=['BLOCK', 'ADDRESS'],\n",
    "    how='left'\n",
    ")\n",
    "test_processed.drop(columns=['ADDRESS'], inplace=True)\n",
    "\n",
    "missing_coords = test_processed[['LATITUDE', 'LONGITUDE']].isnull().any(axis=1).sum()\n",
    "print(f\"HDB data merged. Missing coordinates: {missing_coords}/{len(test_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e0bc1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] Creating floor level derived features...\n",
      " Floor level features created\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Floor Level Derived Features\n",
    "# ============================================\n",
    "print(\"\\n[STEP 5] Creating floor level derived features...\")\n",
    "\n",
    "# FLOOR_LEVEL_RATIO\n",
    "test_processed['FLOOR_LEVEL_RATIO'] = (\n",
    "    test_processed['FLOOR_LEVEL_MID'] / test_processed['MAX_FLOOR']\n",
    ")\n",
    "\n",
    "# FLOOR_LEVEL_CATEGORY (临时用于生成IS_HIGH_FLOOR)\n",
    "test_processed['FLOOR_LEVEL_CATEGORY'] = pd.cut(\n",
    "    test_processed['FLOOR_LEVEL_RATIO'],\n",
    "    bins=[0, 0.33, 0.67, 1.0],\n",
    "    labels=['Low', 'Mid', 'High']\n",
    ")\n",
    "\n",
    "# IS_HIGH_FLOOR\n",
    "test_processed['IS_HIGH_FLOOR'] = (test_processed['FLOOR_LEVEL_CATEGORY'] == 'High').astype(int)\n",
    "\n",
    "# IS_HIGH_FLOOR_IN_PREMIUM_TOWN\n",
    "high_premium_towns = ['sengkang', 'yishun', 'jurong west', 'tampines', 'hougang']\n",
    "test_processed['IS_HIGH_FLOOR_IN_PREMIUM_TOWN'] = (\n",
    "    test_processed['IS_HIGH_FLOOR'] & \n",
    "    test_processed['TOWN'].isin(high_premium_towns)\n",
    ").astype(int)\n",
    "\n",
    "print(f\" Floor level features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "756c80e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] Creating amenity proximity features...\n",
      "  Calculating MRT features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a347699ab8994a90861863c22c43e319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating mall features:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculating Hawker features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e062a583e31f4f2485bb579087afd74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating mall features:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculating Primary school features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98952cda6ed14a5ca21543df2362de2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating mall features:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculating Secondary school features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf90b018e5541f6ace563a80cb5d824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating mall features:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculating Mall features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5629a6c728b9435fbaff83b652e16320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating mall features:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amenity features created. Shape: (50000, 92)\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Amenity Proximity Features\n",
    "# ============================================\n",
    "print(\"\\n[STEP 6] Creating amenity proximity features...\")\n",
    "\n",
    "test_coords = test_processed[['LATITUDE', 'LONGITUDE']].copy()\n",
    "\n",
    "# MRT features\n",
    "print(\"  Calculating MRT features...\")\n",
    "mrt_features = create_auxiliary_location_features(\n",
    "    hdb_coords=test_coords,\n",
    "    auxilliary_df=df_mrt,\n",
    "    feature_prefix='MRT',\n",
    "    radii=[0.5, 1.0, 2.0],\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "# Hawker features\n",
    "print(\"  Calculating Hawker features...\")\n",
    "hawker_features = create_auxiliary_location_features(\n",
    "    hdb_coords=test_coords,\n",
    "    auxilliary_df=df_hawker,\n",
    "    radii=[0.5, 1.5, 3.0],\n",
    "    feature_prefix='HAWKER',\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "# Primary school features\n",
    "print(\"  Calculating Primary school features...\")\n",
    "primary_features = create_auxiliary_location_features(\n",
    "    hdb_coords=test_coords,\n",
    "    auxilliary_df=df_primary,\n",
    "    radii=[1.0, 2.0, 3.0],\n",
    "    feature_prefix='PRIMARY',\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "# Secondary school features\n",
    "print(\"  Calculating Secondary school features...\")\n",
    "secondary_features = create_auxiliary_location_features(\n",
    "    hdb_coords=test_coords,\n",
    "    auxilliary_df=df_secondary,\n",
    "    radii=[1.0, 2.0, 3.0],\n",
    "    feature_prefix='SECONDARY',\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "# Mall features\n",
    "print(\"  Calculating Mall features...\")\n",
    "mall_features = create_auxiliary_location_features(\n",
    "    hdb_coords=test_coords,\n",
    "    auxilliary_df=df_malls,\n",
    "    radii=[1.0, 2.0, 3.0],\n",
    "    feature_prefix='MALL',\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "# Concatenate all proximity features\n",
    "test_processed = test_processed.join([\n",
    "    mrt_features,\n",
    "    hawker_features,\n",
    "    primary_features,\n",
    "    secondary_features,\n",
    "    mall_features\n",
    "])\n",
    "print(f\"Amenity features created. Shape: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c86d509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] Creating KNN geographic features using train data...\n",
      "  Fitting KNN on 162570 training points...\n",
      "  Finding neighbors for 50000 test points...\n",
      "KNN features created for K values: [16, 32, 64, 128]\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: KNN Geographic Features (CRITICAL!)\n",
    "# ============================================\n",
    "print(\"\\n[STEP 7] Creating KNN geographic features using train data...\")\n",
    "\n",
    "# Extract training coordinates and prices\n",
    "X_train_geo = train_geo_info['coordinates']  # Training set coordinates\n",
    "y_train_price = train_geo_info['prices']      # Training set prices\n",
    "K_VALUES = train_geo_info['k_values']         # [16, 32, 64, 128]\n",
    "\n",
    "# Get test coordinates \n",
    "test_has_coords = ~test_processed[['LATITUDE', 'LONGITUDE']].isnull().any(axis=1)\n",
    "X_test_geo = test_processed.loc[test_has_coords, ['LATITUDE', 'LONGITUDE']].values\n",
    "\n",
    "# Initialize KNN feature columns with NaN\n",
    "for k in K_VALUES:\n",
    "    test_processed[f'GEO_AVG_PRICE_K{k}'] = np.nan\n",
    "    test_processed[f'GEO_STD_PRICE_K{k}'] = np.nan\n",
    "\n",
    "# Fit KNN on ENTIRE training set (no cross-validation for test)\n",
    "print(f\"  Fitting KNN on {len(X_train_geo)} training points...\")\n",
    "nn_model = NearestNeighbors(n_neighbors=max(K_VALUES), n_jobs=-1, metric='haversine')\n",
    "nn_model.fit(X_train_geo)\n",
    "\n",
    "# Find neighbors for test data\n",
    "print(f\"  Finding neighbors for {len(X_test_geo)} test points...\")\n",
    "distances, indices = nn_model.kneighbors(X_test_geo)\n",
    "\n",
    "# Generate KNN features for each K value\n",
    "for k in K_VALUES:\n",
    "    k_indices = indices[:, :k]\n",
    "    neighbor_prices = y_train_price[k_indices]\n",
    "    \n",
    "    mean_features = np.mean(neighbor_prices, axis=1)\n",
    "    std_features = np.std(neighbor_prices, axis=1)\n",
    "    \n",
    "    test_processed.loc[test_has_coords, f'GEO_AVG_PRICE_K{k}'] = mean_features\n",
    "    test_processed.loc[test_has_coords, f'GEO_STD_PRICE_K{k}'] = std_features\n",
    "\n",
    "print(f\"KNN features created for K values: {K_VALUES}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dbc86e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 8] Dropping unnecessary columns...\n",
      "✓ Dropped 13 unnecessary columns\n",
      "  Current shape: (50000, 87)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 8: Drop Unnecessary Columns\n",
    "# ============================================\n",
    "print(\"\\n[STEP 8] Dropping unnecessary columns...\")\n",
    "\n",
    "columns_to_drop = [\n",
    "    'MONTH', 'TOWN', 'FLAT_TYPE', 'BLOCK', 'STREET', 'FLOOR_RANGE',\n",
    "    'FLAT_MODEL', 'ECO_CATEGORY', 'LEASE_COMMENCE_DATA',\n",
    "    'FLOOR_LEVEL_LOW', 'FLOOR_LEVEL_HIGH', 'FLOOR_LEVEL_CATEGORY',\n",
    "    'POSTAL_CODE'\n",
    "]\n",
    "\n",
    "test_processed.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "print(f\"✓ Dropped {len(columns_to_drop)} unnecessary columns\")\n",
    "print(f\"  Current shape: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c24017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 9] Aligning with training feature set...\n",
      "Feature alignment completed\n",
      "Final shape: (50000, 87)\n",
      "Expected: (50000, 87)\n"
     ]
    }
   ],
   "source": [
    "# STEP 9: Align with Training Features\n",
    "# ============================================\n",
    "print(\"\\n[STEP 9] Aligning with training feature set...\")\n",
    "\n",
    "# In training dataset, there are RESALE_PRICE and LOG_RESALE_PRICE, but in test dataset, there are not\n",
    "train_only_features = ['RESALE_PRICE', 'LOG_RESALE_PRICE']\n",
    "expected_test_features = [f for f in all_final_features if f not in train_only_features]\n",
    "\n",
    "# Check missing columns\n",
    "missing_features = set(expected_test_features) - set(test_processed.columns)\n",
    "extra_features = set(test_processed.columns) - set(expected_test_features)\n",
    "\n",
    "if missing_features:\n",
    "    print(f\" Missing features: {missing_features}\")\n",
    "    # Add missing columns (fill with 0)\n",
    "    for feat in missing_features:\n",
    "        test_processed[feat] = 0\n",
    "        print(f\" Added {feat} with default value 0\")\n",
    "\n",
    "if extra_features:\n",
    "    print(f\" Extra features (will be dropped): {extra_features}\")\n",
    "    test_processed.drop(columns=list(extra_features), inplace=True)\n",
    "\n",
    "# Reorder columns to match training set (except RESALE_PRICE and LOG_RESALE_PRICE)\n",
    "test_processed = test_processed[expected_test_features]\n",
    "\n",
    "print(f\"Feature alignment completed\")\n",
    "print(f\"Final shape: {test_processed.shape}\")\n",
    "print(f\"Expected: ({len(test_df)}, {len(expected_test_features)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63097b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 10] Final validation and saving...\n",
      "   No missing values detected\n",
      "\n",
      " Test data saved to: ../Dataset/test_data_for_modeling(no_standardization).csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 10: Final Validation and Save (without standardization)\n",
    "# ============================================\n",
    "print(\"\\n[STEP 10] Final validation and saving...\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_summary = test_processed.isnull().sum()\n",
    "missing_cols = missing_summary[missing_summary > 0]\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"  WARNING: {len(missing_cols)} columns have missing values:\")\n",
    "    print(missing_cols)\n",
    "    print(\"\\n  Handling missing values...\")\n",
    "    \n",
    "    # For KNN features, if missing, fill with overall mean\n",
    "    knn_cols = [col for col in test_processed.columns if 'GEO_' in col]\n",
    "    for col in knn_cols:\n",
    "        if test_processed[col].isnull().any():\n",
    "            # Note: Here we use the mean of the training set, not the test set!\n",
    "            train_median=knn_feature_stats[col]['median']\n",
    "            test_processed[col].fillna(train_median, inplace=True)\n",
    "            print(f\"    Filled {col} with train median: {train_median:.2f}\")\n",
    "    \n",
    "    # For other features, fill with 0\n",
    "    test_processed.fillna(0, inplace=True)\n",
    "    print(f\"  Missing values handled\")\n",
    "else:\n",
    "    print(f\"   No missing values detected\")\n",
    "\n",
    "# Save processed test data for Tree-based Model (without standardization)\n",
    "output_path = '../Dataset/test_data_for_modeling(no_standardization).csv'\n",
    "test_processed.to_csv(output_path, index=False)\n",
    "print(f\"\\n Test data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a7505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 11] Applying standardization ...\n",
      "Standardizing 36 numerical features...\n",
      "Standardization applied using saved scaler\n"
     ]
    }
   ],
   "source": [
    "# STEP 11: Apply Standardization\n",
    "# ============================================\n",
    "print(\"\\n[STEP 11] Applying standardization ...\")\n",
    "\n",
    "numerical_features_in_test = [f for f in numerical_features if f in test_processed.columns]\n",
    "\n",
    "print(f\"Standardizing {len(numerical_features_in_test)} numerical features...\")\n",
    "test_processed[numerical_features_in_test] = scaler.transform(\n",
    "    test_processed[numerical_features_in_test]\n",
    ")\n",
    "\n",
    "print(f\"Standardization applied using saved scaler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3b691bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 11] Final validation and saving...\n",
      "   No missing values detected\n",
      "\n",
      " Test data saved to: ../Dataset/test_data_for_modeling.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 12: Final Validation and Save\n",
    "# ============================================\n",
    "print(\"\\n[STEP 11] Final validation and saving...\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_summary = test_processed.isnull().sum()\n",
    "missing_cols = missing_summary[missing_summary > 0]\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"  WARNING: {len(missing_cols)} columns have missing values:\")\n",
    "    print(missing_cols)\n",
    "    print(\"\\n  Handling missing values...\")\n",
    "    \n",
    "    # For KNN features, if missing, fill with overall mean\n",
    "    knn_cols = [col for col in test_processed.columns if 'GEO_' in col]\n",
    "    for col in knn_cols:\n",
    "        if test_processed[col].isnull().any():\n",
    "            # Note: Here we use the mean of the training set, not the test set!\n",
    "            train_median=knn_feature_stats[col]['median']\n",
    "            test_processed[col].fillna(train_median, inplace=True)\n",
    "            print(f\"    Filled {col} with train median: {train_median:.2f}\")\n",
    "    \n",
    "    # For other features, fill with 0\n",
    "    test_processed.fillna(0, inplace=True)\n",
    "    print(f\"  Missing values handled\")\n",
    "else:\n",
    "    print(f\"   No missing values detected\")\n",
    "\n",
    "# Save processed test data\n",
    "output_path = '../Dataset/test_data_for_modeling.csv'\n",
    "test_processed.to_csv(output_path, index=False)\n",
    "print(f\"\\n Test data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c93025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST DATA PROCESSING COMPLETED SUCCESSFULLY!\n",
      "Original test data:     (50000, 10)\n",
      "Processed test data:    (50000, 87)\n",
      "Expected features:      87\n",
      "Actual features:        87\n",
      "Match:                  True\n",
      "\n",
      "Feature categories:\n",
      "  - Numerical features:    36\n",
      "  - OneHot FLAT_MODEL:     21\n",
      "  - OneHot TOWN:           26\n",
      "  - KNN features:          8\n",
      "  - Amenity features:      20\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST DATA PROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"Original test data:     {test_df.shape}\")\n",
    "print(f\"Processed test data:    {test_processed.shape}\")\n",
    "print(f\"Expected features:      {len(expected_test_features)}\")\n",
    "print(f\"Actual features:        {test_processed.shape[1]}\")\n",
    "print(f\"Match:                  {test_processed.shape[1] == len(expected_test_features)}\")\n",
    "print(\"\\nFeature categories:\")\n",
    "print(f\"  - Numerical features:    {len(numerical_features_in_test)}\")\n",
    "print(f\"  - OneHot FLAT_MODEL:     {len([c for c in test_processed.columns if 'FLAT_MODEL_' in c])}\")\n",
    "print(f\"  - OneHot TOWN:           {len([c for c in test_processed.columns if 'TOWN_' in c])}\")\n",
    "print(f\"  - KNN features:          {len([c for c in test_processed.columns if 'GEO_' in c])}\")\n",
    "print(f\"  - Amenity features:      {len([c for c in test_processed.columns if 'NEAREST_' in c or 'COUNT_' in c])}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS5228env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
